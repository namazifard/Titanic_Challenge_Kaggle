{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Titanic Project Challenge"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["import numpy as np \n","import pandas as pd \n","import seaborn as sns \n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["training = pd.read_csv('/kaggle/input/titanic/train.csv')\n","test = pd.read_csv('/kaggle/input/titanic/test.csv')\n","\n","training['train_test'] = 1\n","test['train_test'] = 0\n","test['Survived'] = np.NaN\n","all_data = pd.concat([training,test])\n","\n","%matplotlib inline\n","all_data.columns"]},{"cell_type":"markdown","metadata":{},"source":["## Light Data Exploration\n","### 1) For numeric data \n","* Made histograms to understand distributions \n","* Corrplot \n","* Pivot table comparing survival rate across numeric variables \n","\n","\n","### 2) For Categorical Data \n","* Made bar charts to understand balance of classes \n","* Made pivot tables to understand relationship with survival "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#quick look at our data types & null counts \n","training.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# to better understand the numeric data, we want to use the .describe() method. This gives us an understanding of the central tendencies of the data \n","training.describe()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#quick way to separate numeric columns\n","training.describe().columns"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# look at numeric and categorical values separately \n","df_num = training[['Age','SibSp','Parch','Fare']]\n","df_cat = training[['Survived','Pclass','Sex','Ticket','Cabin','Embarked']]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#distributions for all numeric variables \n","for i in df_num.columns:\n","    plt.hist(df_num[i])\n","    plt.title(i)\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(df_num.corr())\n","sns.heatmap(df_num.corr())"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# compare survival rate across Age, SibSp, Parch, and Fare \n","pd.pivot_table(training, index = 'Survived', values = ['Age','SibSp','Parch','Fare'])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["for i in df_cat.columns:\n","    sns.barplot(df_cat[i].value_counts().index,df_cat[i].value_counts()).set_title(i)\n","    plt.show()\\\n","    "]},{"cell_type":"markdown","metadata":{},"source":["Cabin and ticket graphs are very messy. This is an area where we may want to do some feature engineering! "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Comparing survival and each of these categorical variables \n","print(pd.pivot_table(training, index = 'Survived', columns = 'Pclass', values = 'Ticket' ,aggfunc ='count'))\n","print()\n","print(pd.pivot_table(training, index = 'Survived', columns = 'Sex', values = 'Ticket' ,aggfunc ='count'))\n","print()\n","print(pd.pivot_table(training, index = 'Survived', columns = 'Embarked', values = 'Ticket' ,aggfunc ='count'))"]},{"cell_type":"markdown","metadata":{},"source":["## Feature Engineering \n","### 1) Cabin - Simplify cabins (evaluated if cabin letter (cabin_adv) or the purchase of tickets across multiple cabins (cabin_multiple) impacted survival)\n","\n","### 2) Tickets - Do different ticket types impact survival rates?\n","\n","### 3) Does a person's title relate to survival rates? "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df_cat.Cabin\n","training['cabin_multiple'] = training.Cabin.apply(lambda x: 0 if pd.isna(x) else len(x.split(' ')))\n","# after looking at this, we may want to look at cabin by letter or by number. Let's create some categories for this \n","# letters \n","# multiple letters \n","training['cabin_multiple'].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["pd.pivot_table(training, index = 'Survived', columns = 'cabin_multiple', values = 'Ticket' ,aggfunc ='count')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#creates categories based on the cabin letter (n stands for null)\n","#in this case we will treat null values like it's own category\n","\n","training['cabin_adv'] = training.Cabin.apply(lambda x: str(x)[0])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#comparing surivial rate by cabin\n","print(training.cabin_adv.value_counts())\n","pd.pivot_table(training,index='Survived',columns='cabin_adv', values = 'Name', aggfunc='count')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#understand ticket values better \n","#numeric vs non numeric \n","training['numeric_ticket'] = training.Ticket.apply(lambda x: 1 if x.isnumeric() else 0)\n","training['ticket_letters'] = training.Ticket.apply(lambda x: ''.join(x.split(' ')[:-1]).replace('.','').replace('/','').lower() if len(x.split(' ')[:-1]) >0 else 0)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["training['numeric_ticket'].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#lets us view all rows in dataframe through scrolling. This is for convenience \n","pd.set_option(\"max_rows\", None)\n","training['ticket_letters'].value_counts()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#difference in numeric vs non-numeric tickets in survival rate \n","pd.pivot_table(training,index='Survived',columns='numeric_ticket', values = 'Ticket', aggfunc='count')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#survival rate across different tyicket types \n","pd.pivot_table(training,index='Survived',columns='ticket_letters', values = 'Ticket', aggfunc='count')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#feature engineering on person's title \n","training.Name.head(50)\n","training['name_title'] = training.Name.apply(lambda x: x.split(',')[1].split('.')[0].strip())\n","#mr., ms., master. etc"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["training['name_title'].value_counts()"]},{"cell_type":"markdown","metadata":{},"source":["## Data Preprocessing for Model \n","### 1) Drop null values from Embarked (only 2) \n","\n","### 2) Include only relevant variables (Since we have limited data, I wanted to exclude things like name and passanger ID so that we could have a reasonable number of features for our models to deal with) \n","Variables:  'Pclass', 'Sex','Age', 'SibSp', 'Parch', 'Fare', 'Embarked', 'cabin_adv', 'cabin_multiple', 'numeric_ticket', 'name_title'\n","\n","### 3) Do categorical transforms on all data. Usually we would use a transformer, but with this approach we can ensure that our traning and test data have the same colums. We also may be able to infer something about the shape of the test data through this method. I will stress, this is generally not recommend outside of a competition (use onehot encoder). \n","\n","### 4) Impute data with mean for fare and age (Should also experiment with median) \n","\n","### 5) Normalized fare using logarithm to give more semblance of a normal distribution \n","\n","### 6) Scaled data 0-1 with standard scaler "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#create all categorical variables that we did above for both training and test sets \n","all_data['cabin_multiple'] = all_data.Cabin.apply(lambda x: 0 if pd.isna(x) else len(x.split(' ')))\n","all_data['cabin_adv'] = all_data.Cabin.apply(lambda x: str(x)[0])\n","all_data['numeric_ticket'] = all_data.Ticket.apply(lambda x: 1 if x.isnumeric() else 0)\n","all_data['ticket_letters'] = all_data.Ticket.apply(lambda x: ''.join(x.split(' ')[:-1]).replace('.','').replace('/','').lower() if len(x.split(' ')[:-1]) >0 else 0)\n","all_data['name_title'] = all_data.Name.apply(lambda x: x.split(',')[1].split('.')[0].strip())\n","\n","#impute nulls for continuous data \n","#all_data.Age = all_data.Age.fillna(training.Age.mean())\n","all_data.Age = all_data.Age.fillna(training.Age.median())\n","#all_data.Fare = all_data.Fare.fillna(training.Fare.mean())\n","all_data.Fare = all_data.Fare.fillna(training.Fare.median())\n","\n","#drop null 'embarked' rows. Only 2 instances of this in training and 0 in test \n","all_data.dropna(subset=['Embarked'],inplace = True)\n","\n","#tried log norm of sibsp (not used)\n","all_data['norm_sibsp'] = np.log(all_data.SibSp+1)\n","all_data['norm_sibsp'].hist()\n","\n","# log norm of fare (used)\n","all_data['norm_fare'] = np.log(all_data.Fare+1)\n","all_data['norm_fare'].hist()\n","\n","# converted fare to category for pd.get_dummies()\n","all_data.Pclass = all_data.Pclass.astype(str)\n","\n","#created dummy variables from categories (also can use OneHotEncoder)\n","all_dummies = pd.get_dummies(all_data[['Pclass','Sex','Age','SibSp','Parch','norm_fare','Embarked','cabin_adv','cabin_multiple','numeric_ticket','name_title','train_test']])\n","\n","#Split to train test again\n","X_train = all_dummies[all_dummies.train_test == 1].drop(['train_test'], axis =1)\n","X_test = all_dummies[all_dummies.train_test == 0].drop(['train_test'], axis =1)\n","\n","\n","y_train = all_data[all_data.train_test==1].Survived\n","y_train.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Scale data \n","from sklearn.preprocessing import StandardScaler\n","scale = StandardScaler()\n","all_dummies_scaled = all_dummies.copy()\n","all_dummies_scaled[['Age','SibSp','Parch','norm_fare']]= scale.fit_transform(all_dummies_scaled[['Age','SibSp','Parch','norm_fare']])\n","all_dummies_scaled\n","\n","X_train_scaled = all_dummies_scaled[all_dummies_scaled.train_test == 1].drop(['train_test'], axis =1)\n","X_test_scaled = all_dummies_scaled[all_dummies_scaled.train_test == 0].drop(['train_test'], axis =1)\n","\n","y_train = all_data[all_data.train_test==1].Survived\n"]},{"cell_type":"markdown","metadata":{},"source":["## Model Building (Baseline Validation Performance)\n","Before going further, I like to see how various different models perform with default parameters. I tried the following models using 5 fold cross validation to get a baseline. With a validation set basline, we can see how much tuning improves each of the models. Just because a model has a high basline on this validation set doesn't mean that it will actually do better on the eventual test set. \n","\n","- Naive Bayes (72.6%)\n","- Logistic Regression (82.1%)\n","- Decision Tree (77.6%)\n","- K Nearest Neighbor (80.5%)\n","- Random Forest (80.6%)\n","- **Support Vector Classifier (83.2%)**\n","- Xtreme Gradient Boosting (81.8%)\n","- Soft Voting Classifier - All Models (82.8%)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn.model_selection import cross_val_score\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.linear_model import LogisticRegression\n","from sklearn import tree\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.svm import SVC"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#I usually use Naive Bayes as a baseline for my classification tasks \n","gnb = GaussianNB()\n","cv = cross_val_score(gnb,X_train_scaled,y_train,cv=5)\n","print(cv)\n","print(cv.mean())"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["lr = LogisticRegression(max_iter = 2000)\n","cv = cross_val_score(lr,X_train,y_train,cv=5)\n","print(cv)\n","print(cv.mean())"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["lr = LogisticRegression(max_iter = 2000)\n","cv = cross_val_score(lr,X_train_scaled,y_train,cv=5)\n","print(cv)\n","print(cv.mean())"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["dt = tree.DecisionTreeClassifier(random_state = 1)\n","cv = cross_val_score(dt,X_train,y_train,cv=5)\n","print(cv)\n","print(cv.mean())"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["dt = tree.DecisionTreeClassifier(random_state = 1)\n","cv = cross_val_score(dt,X_train_scaled,y_train,cv=5)\n","print(cv)\n","print(cv.mean())"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["knn = KNeighborsClassifier()\n","cv = cross_val_score(knn,X_train,y_train,cv=5)\n","print(cv)\n","print(cv.mean())"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["knn = KNeighborsClassifier()\n","cv = cross_val_score(knn,X_train_scaled,y_train,cv=5)\n","print(cv)\n","print(cv.mean())"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["rf = RandomForestClassifier(random_state = 1)\n","cv = cross_val_score(rf,X_train,y_train,cv=5)\n","print(cv)\n","print(cv.mean())"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["rf = RandomForestClassifier(random_state = 1)\n","cv = cross_val_score(rf,X_train_scaled,y_train,cv=5)\n","print(cv)\n","print(cv.mean())"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["svc = SVC(probability = True)\n","cv = cross_val_score(svc,X_train_scaled,y_train,cv=5)\n","print(cv)\n","print(cv.mean())"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from xgboost import XGBClassifier\n","xgb = XGBClassifier(random_state =1)\n","cv = cross_val_score(xgb,X_train_scaled,y_train,cv=5)\n","print(cv)\n","print(cv.mean())"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn.ensemble import VotingClassifier\n","voting_clf = VotingClassifier(estimators = [('lr',lr),('knn',knn),('rf',rf),('gnb',gnb),('svc',svc),('xgb',xgb)], voting = 'soft') "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["cv = cross_val_score(voting_clf,X_train_scaled,y_train,cv=5)\n","print(cv)\n","print(cv.mean())"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["voting_clf.fit(X_train_scaled,y_train)\n","y_hat_base_vc = voting_clf.predict(X_test_scaled).astype(int)\n","basic_submission = {'PassengerId': test.PassengerId, 'Survived': y_hat_base_vc}\n","base_submission = pd.DataFrame(data=basic_submission)\n","base_submission.to_csv('base_submission.csv', index=False)"]},{"cell_type":"markdown","metadata":{},"source":["## Model Tuned Performance \n","After getting the baselines, let's see if we can improve on the indivdual model results!I mainly used grid search to tune the models. I also used Randomized Search for the Random Forest and XG boosted model to simplify testing time. \n","\n","|Model|Baseline|Tuned Performance|\n","|-----|--------|-----------------|\n","|Naive Bayes| 72.6%| NA|\n","|Logistic Regression| 82.1%| 82.6%|\n","|Decision Tree| 77.6%| NA|\n","|K Nearest Neighbor| 80.5%|83.0%|\n","|Random Forest| 80.6%| 83.6|\n","|Support Vector Classifier| 83.2%| 83.2%|\n","|Xtreme Gradient Boosting| 81.8%| 85.3%|\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn.model_selection import GridSearchCV \n","from sklearn.model_selection import RandomizedSearchCV "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#simple performance reporting function\n","def clf_performance(classifier, model_name):\n","    print(model_name)\n","    print('Best Score: ' + str(classifier.best_score_))\n","    print('Best Parameters: ' + str(classifier.best_params_))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["lr = LogisticRegression()\n","param_grid = {'max_iter' : [2000],\n","              'penalty' : ['l1', 'l2'],\n","              'C' : np.logspace(-4, 4, 20),\n","              'solver' : ['liblinear']}\n","\n","clf_lr = GridSearchCV(lr, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\n","best_clf_lr = clf_lr.fit(X_train_scaled,y_train)\n","clf_performance(best_clf_lr,'Logistic Regression')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["knn = KNeighborsClassifier()\n","param_grid = {'n_neighbors' : [3,5,7,9],\n","              'weights' : ['uniform', 'distance'],\n","              'algorithm' : ['auto', 'ball_tree','kd_tree'],\n","              'p' : [1,2]}\n","clf_knn = GridSearchCV(knn, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\n","best_clf_knn = clf_knn.fit(X_train_scaled,y_train)\n","clf_performance(best_clf_knn,'KNN')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["svc = SVC(probability = True)\n","param_grid = tuned_parameters = [{'kernel': ['rbf'], 'gamma': [.1,.5,1,2,5,10],\n","                                  'C': [.1, 1, 10, 100, 1000]},\n","                                 {'kernel': ['linear'], 'C': [.1, 1, 10, 100, 1000]},\n","                                 {'kernel': ['poly'], 'degree' : [2,3,4,5], 'C': [.1, 1, 10, 100, 1000]}]\n","clf_svc = GridSearchCV(svc, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\n","best_clf_svc = clf_svc.fit(X_train_scaled,y_train)\n","clf_performance(best_clf_svc,'SVC')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["rf = RandomForestClassifier(random_state = 1)\n","param_grid =  {'n_estimators': [400,450,500,550],\n","               'criterion':['gini','entropy'],\n","                                  'bootstrap': [True],\n","                                  'max_depth': [15, 20, 25],\n","                                  'max_features': ['auto','sqrt', 10],\n","                                  'min_samples_leaf': [2,3],\n","                                  'min_samples_split': [2,3]}\n","                                  \n","clf_rf = GridSearchCV(rf, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\n","best_clf_rf = clf_rf.fit(X_train_scaled,y_train)\n","clf_performance(best_clf_rf,'Random Forest')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["best_rf = best_clf_rf.best_estimator_.fit(X_train_scaled,y_train)\n","feat_importances = pd.Series(best_rf.feature_importances_, index=X_train_scaled.columns)\n","feat_importances.nlargest(20).plot(kind='barh')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["xgb = XGBClassifier(random_state = 1)\n","\n","param_grid = {\n","    'n_estimators': [450,500,550],\n","    'colsample_bytree': [0.75,0.8,0.85],\n","    'max_depth': [None],\n","    'reg_alpha': [1],\n","    'reg_lambda': [2, 5, 10],\n","    'subsample': [0.55, 0.6, .65],\n","    'learning_rate':[0.5],\n","    'gamma':[.5,1,2],\n","    'min_child_weight':[0.01],\n","    'sampling_method': ['uniform']\n","}\n","\n","clf_xgb = GridSearchCV(xgb, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\n","best_clf_xgb = clf_xgb.fit(X_train_scaled,y_train)\n","clf_performance(best_clf_xgb,'XGB')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["y_hat_xgb = best_clf_xgb.best_estimator_.predict(X_test_scaled).astype(int)\n","xgb_submission = {'PassengerId': test.PassengerId, 'Survived': y_hat_xgb}\n","submission_xgb = pd.DataFrame(data=xgb_submission)\n","submission_xgb.to_csv('xgb_submission3.csv', index=False)"]},{"cell_type":"markdown","metadata":{},"source":["## Model Additional Ensemble Approaches \n","1) Experimented with a hard voting classifier of three estimators (KNN, SVM, RF) (81.6%)\n","\n","2) **Experimented with a soft voting classifier of three estimators (KNN, SVM, RF) (82.3%) (Best Performance)**\n","\n","3) Experimented with soft voting on all estimators performing better than 80% except xgb (KNN, RF, LR, SVC) (82.9%)\n","\n","4) Experimented with soft voting on all estimators including XGB (KNN, SVM, RF, LR, XGB) (83.5%)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["best_lr = best_clf_lr.best_estimator_\n","best_knn = best_clf_knn.best_estimator_\n","best_svc = best_clf_svc.best_estimator_\n","best_rf = best_clf_rf.best_estimator_\n","best_xgb = best_clf_xgb.best_estimator_\n","\n","voting_clf_hard = VotingClassifier(estimators = [('knn',best_knn),('rf',best_rf),('svc',best_svc)], voting = 'hard') \n","voting_clf_soft = VotingClassifier(estimators = [('knn',best_knn),('rf',best_rf),('svc',best_svc)], voting = 'soft') \n","voting_clf_all = VotingClassifier(estimators = [('knn',best_knn),('rf',best_rf),('svc',best_svc), ('lr', best_lr)], voting = 'soft') \n","voting_clf_xgb = VotingClassifier(estimators = [('knn',best_knn),('rf',best_rf),('svc',best_svc), ('xgb', best_xgb),('lr', best_lr)], voting = 'soft')\n","\n","print('voting_clf_hard :',cross_val_score(voting_clf_hard,X_train,y_train,cv=5))\n","print('voting_clf_hard mean :',cross_val_score(voting_clf_hard,X_train,y_train,cv=5).mean())\n","\n","print('voting_clf_soft :',cross_val_score(voting_clf_soft,X_train,y_train,cv=5))\n","print('voting_clf_soft mean :',cross_val_score(voting_clf_soft,X_train,y_train,cv=5).mean())\n","\n","print('voting_clf_all :',cross_val_score(voting_clf_all,X_train,y_train,cv=5))\n","print('voting_clf_all mean :',cross_val_score(voting_clf_all,X_train,y_train,cv=5).mean())\n","\n","print('voting_clf_xgb :',cross_val_score(voting_clf_xgb,X_train,y_train,cv=5))\n","print('voting_clf_xgb mean :',cross_val_score(voting_clf_xgb,X_train,y_train,cv=5).mean())\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#in a soft voting classifier you can weight some models more than others. I used a grid search to explore different weightings\n","#no new results here\n","params = {'weights' : [[1,1,1],[1,2,1],[1,1,2],[2,1,1],[2,2,1],[1,2,2],[2,1,2]]}\n","\n","vote_weight = GridSearchCV(voting_clf_soft, param_grid = params, cv = 5, verbose = True, n_jobs = -1)\n","best_clf_weight = vote_weight.fit(X_train_scaled,y_train)\n","clf_performance(best_clf_weight,'VC Weights')\n","voting_clf_sub = best_clf_weight.best_estimator_.predict(X_test_scaled)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#Make Predictions \n","voting_clf_hard.fit(X_train_scaled, y_train)\n","voting_clf_soft.fit(X_train_scaled, y_train)\n","voting_clf_all.fit(X_train_scaled, y_train)\n","voting_clf_xgb.fit(X_train_scaled, y_train)\n","\n","best_rf.fit(X_train_scaled, y_train)\n","y_hat_vc_hard = voting_clf_hard.predict(X_test_scaled).astype(int)\n","y_hat_rf = best_rf.predict(X_test_scaled).astype(int)\n","y_hat_vc_soft =  voting_clf_soft.predict(X_test_scaled).astype(int)\n","y_hat_vc_all = voting_clf_all.predict(X_test_scaled).astype(int)\n","y_hat_vc_xgb = voting_clf_xgb.predict(X_test_scaled).astype(int)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#convert output to dataframe \n","final_data = {'PassengerId': test.PassengerId, 'Survived': y_hat_rf}\n","submission = pd.DataFrame(data=final_data)\n","\n","final_data_2 = {'PassengerId': test.PassengerId, 'Survived': y_hat_vc_hard}\n","submission_2 = pd.DataFrame(data=final_data_2)\n","\n","final_data_3 = {'PassengerId': test.PassengerId, 'Survived': y_hat_vc_soft}\n","submission_3 = pd.DataFrame(data=final_data_3)\n","\n","final_data_4 = {'PassengerId': test.PassengerId, 'Survived': y_hat_vc_all}\n","submission_4 = pd.DataFrame(data=final_data_4)\n","\n","final_data_5 = {'PassengerId': test.PassengerId, 'Survived': y_hat_vc_xgb}\n","submission_5 = pd.DataFrame(data=final_data_5)\n","\n","final_data_comp = {'PassengerId': test.PassengerId, 'Survived_vc_hard': y_hat_vc_hard, 'Survived_rf': y_hat_rf, 'Survived_vc_soft' : y_hat_vc_soft, 'Survived_vc_all' : y_hat_vc_all,  'Survived_vc_xgb' : y_hat_vc_xgb}\n","comparison = pd.DataFrame(data=final_data_comp)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#track differences between outputs \n","comparison['difference_rf_vc_hard'] = comparison.apply(lambda x: 1 if x.Survived_vc_hard != x.Survived_rf else 0, axis =1)\n","comparison['difference_soft_hard'] = comparison.apply(lambda x: 1 if x.Survived_vc_hard != x.Survived_vc_soft else 0, axis =1)\n","comparison['difference_hard_all'] = comparison.apply(lambda x: 1 if x.Survived_vc_all != x.Survived_vc_hard else 0, axis =1)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["comparison.difference_hard_all.value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#prepare submission files \n","submission.to_csv('submission_rf.csv', index =False)\n","submission_2.to_csv('submission_vc_hard.csv',index=False)\n","submission_3.to_csv('submission_vc_soft.csv', index=False)\n","submission_4.to_csv('submission_vc_all.csv', index=False)\n","submission_5.to_csv('submission_vc_xgb2.csv', index=False)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
